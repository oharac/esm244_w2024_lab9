---
title: 'ESM 244 Lab Week 6: Clustering (k-means & hierarchical)'
format: 
  html:
    code-fold: show
    embed-resources: true
execute:
  message: false
  warning: false
---



In this lab, you'll learn how to do some cluster exploration by partition-based (k-means) and hierarchical clustering.

## Get & attach required packages

Note: You'll probably need to install the last 5 packages here for clustering. 

```{r}
library(tidyverse)

# Packages for cluster analysis:
library(NbClust)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
```


# Part 1. K-means clustering: 

To practice k-means clustering, we'll use the [wheat seeds dataset](https://archive.ics.uci.edu/dataset/236/seeds) from UC Irvine's Machine Learning Repository.  This was featured in:

* M. Charytanowicz, J. Niewczas, P. Kulczycki, Piotr A. Kowalski, Szymon Łukasik, Slawomir Zak. 2010 [Complete Gradient Clustering Algorithm for Features Analysis of X-Ray Images](https://www.semanticscholar.org/paper/Complete-Gradient-Clustering-Algorithm-for-Features-Charytanowicz-Niewczas/24a9453d3cab64995e32506f884c2a1792a6d4ca).  Information Technologies in Biomedicine.

From the repository:

> Measurements of geometrical properties of kernels belonging to three different varieties of wheat. A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.
>
> The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.
> 
> The data set can be used for the tasks of classification and cluster analysis.

Variables:

1. area A, 
2. perimeter P, 
3. compactness C = 4*pi*A/P^2, 
4. length of kernel,
5. width of kernel,
6. asymmetry coefficient
7. length of kernel groove.
8. variety: Kama=1, Rosa=2, Canadian=3

All of these parameters were real-valued continuous.

Note: I've slightly modified the dataset to include some NA values...

## Read in the data

This data is in a different format than we are used to.  It is a text file, rather than csv; the columns are separated by tabs, not commas; and the columns are not named so we'll have to name them manually.

```{r}
var_names <- c('a', 'p', 'c', 'l_k', 'w_k', 'asym', 'l_g', 'variety')
seeds_df <- read_tsv(here::here('data/seeds_dataset.txt'),
                     col_names = FALSE,
                     na = '-999') %>%
  setNames(var_names) %>%
  mutate(variety = case_when(variety == 1 ~ 'Kama',
                             variety == 2 ~ 'Rosa',
                             variety == 3 ~ 'Canadian',
                             TRUE ~ 'oops'))

### or names(seeds_df) <- var_names

summary(seeds_df)
```


## Exploratory visualization

Let's examine the distribution of the variables:

```{r}
seeds_df_long <- seeds_df %>%
  pivot_longer(cols = -variety)
ggplot(seeds_df_long, aes(x = value)) +
  geom_histogram() +
  facet_grid(variety ~ name, scales = 'free')
```

Then, do some exploratory data visualization. Does it look like there are any obvious clusters? 

```{r}
ggplot(seeds_df) +
  geom_point(aes(x = a, y = asym, color = c, shape = variety),
             size = 3, alpha = 0.7)

# try other variations: 
ggplot(seeds_df) +
  geom_point(aes(x = l_g, y = w_k, color = asym, shape = variety),
             size = 3, alpha = 0.7)
```


## Pseudocode

From lecture, what are some of the considerations we will need to take into account?  Data wrangling, the clustering process?








## Pick the number of clusters

In the lecture, you learned that for k-means clustering you need to specify the number of clusters *a priori*. R **does** have some tools to help you decide, but this should NOT override your judgement based on conceptual or expert understanding. 

Here, we use the `NbClust::NbClust()` function, which "provides 30 indices for determining the number of clusters and proposes to user the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods". See `?NbClust` for more information. 

Basically, it's going to run 30 different ways of evaluating how many clusters it *thinks* exist, then tell you the breakdown of what they decide (e.g. "8 algorithms think that there should be 4 clusters"). 

## Create a complete, scaled version of the data

We are going to do this with *complete cases* - in other words, for the variables we're using to perform k-means clustering on, we are *dropping any observation (row) where any of those are missing*. Keep in mind that this may not be the best option for every scenario - in other cases (e.g. when we have a large proportion of missingness), we may want to impute missing values instead.

```{r}
# Drop rows where any of the measurements are missing
seeds_complete <- seeds_df %>% 
  drop_na()

# Only keep the columns for the measurements, then SCALE them
seeds_scale <- seeds_complete %>% 
  select(-variety) %>% 
  scale() # See ?scale for details on scaling

# compare scaled to original vars
summary(seeds_complete)
summary(seeds_scale)
```

Make sure to check out what `seeds_scale` looks like!

## identifying optimal number of clusters

Note that we're only using the seven measurement variables from the dataset. We also specify the minimum and maximum number of clusters we want `NbClust` to consider:

```{r}
# How many clusters do you THINK there should be? 
number_est <- NbClust(seeds_scale, min.nc = 2, max.nc = 10, method = "kmeans")

# Check out the results (just look at the first summary report):
number_est

# By these estimators, 3 is identified as the best number of clusters by the largest number of algorithms (11 / 23)...  could we override this?  here I think it makes sense to stick with 3 (a cluster for each variety) and see how it does. 

### knee method
fviz_nbclust(seeds_scale, FUNcluster = kmeans, method = 'wss', k.max = 10)

```

We're going to use 3 clusters and see how it does, though there may be a case here for 2 given that nearly as many of the indices indicated that as the best number. 


## Run k-means 

Now that we have complete, scaled data for the four size variables of interest, let's run k-means. You should know the iterative process it's running through from the Week 6 lecture.  

```{r}
set.seed(10101)
seeds_km <- kmeans(seeds_scale, 3, nstart = 25) # kmeans specifying 3 groups to start
```

Check out the outputs in `seeds_km`: 

```{r}
# See what it returns (different elements returned by kmeans function):
seeds_km$size # How many observations assigned to each cluster
seeds_km$cluster # What cluster each observation in seeds_scale is assigned to

# Bind the cluster number to the original data used for clustering, so that we can see what cluster each variety is assigned to
seeds_cl <- data.frame(seeds_complete, 
                       cluster_no = factor(seeds_km$cluster))

```

``` {r}
### On your own:
### Plot area and asymmetric index, and include cluster number and variety for comparison:

ggplot(seeds_cl) +
  geom_point(aes(x = a, y = asym, color = cluster_no, shape = variety), 
             size = 2)

```

What do we see from this graph? 

We see that a lot of Kama variety (triangles) are in cluster 2 (green), Rosa (squares) in cluster 3 (blue), Canadian (circles) in cluster 1 (red)...  but what are the actual counts? Let's find them: 

```{r}
### how well does this clustering match up to variety?  Select the variety and 
### cluster number vars and make into a continency table
seeds_cl %>% select(variety, cluster_no) %>% table()

```

Takeaway: as we see from the graph, *most* wheat varieties in their own cluster k-means clustering. So this actually does a somewhat decent job of splitting up the three varieties into different clusters, with some overlap here and there, which is consistent with what we observed in exploratory data visualization. 

That's your intro to k-means clustering with wheat seeds! 

# Part 2. Cluster analysis: hierarchical

In this section, we'll be performing hierarchical cluster analysis (& making dendrograms) in R. From lecture you should understand agglomerative versus divisive clustering, as well as differences in linkages (complete, single, average). 

We will use the `stats::hclust()` function for agglomerative hierarchical clustering, first checking how well our clusters compare to using WorldBank environmental data (simplified), wb_env.csv.

## Compare hclust results to kmeans using wheat seeds

Start with complete linkage...  First step, create a Euclidean distance matrix
```{r}
seeds_dist <- dist(seeds_scale, method = 'euclidean') ### look at upper and diag arguments

# Hierarchical clustering (complete linkage)
seeds_hc_complete <- hclust(seeds_dist, method = "complete")

# Plot it (base plot):
plot(seeds_hc_complete, cex = 0.6, hang = -1)
```

```{r}
# use cutree to slice it into three clusters
seeds_cut_hc <- cutree(seeds_hc_complete, 3)
table(seeds_cut_hc, seeds_complete$variety)
```

What if we tried single, average, or ward.D linkage?

How is clustering in this way (either k-means or hierarchical) different from classification using something like binary logistic regression?

* Classification sorts data into categories using a labeled dataset (supervised ML).  This can be used to create predictive models and/or see how predictor variables relate to the outcome variable.
* Clustering sorts an unlabeled dataset into groups of similar observations (unsupervised ML).  There is no prediction target - the goal is to understand the underlying structure.  
* Often, resulting clusters can have some physical interpretation (e.g., penguin species, wheat varieties) but not necessarily - however, the clusters can still provide useful information, e.g., taxonomy of voting patterns or environmental preferences.


## World Bank data: Read in the data, & simplify

Here, we'll read in the WorldBank environmental data (simplified), and keep only the top 20 GHG emitters for this dataset. 

```{r}

# Get the data
wb_env <- read_csv(here::here("data/wb_env.csv"))

# Make sure to take a look at the data:
# View(wb_env)

```

## Pseudocode




## Wrangle the data

```{r}
# Only keep top 20 greenhouse gas emitters (for simplifying visualization here...)
wb_ghg_20 <- wb_env %>% 
  slice_max(ghg, n = 20)

summary(wb_ghg_20)
```

## Scale the data

```{r}
# Scale the numeric variables (columns 3:7)
wb_scaled <- wb_ghg_20 %>% 
  select(3:7) %>% 
  scale()
summary(wb_scaled)

# Update to add rownames (country name) from wb_ghg_20
rownames(wb_scaled) <- wb_ghg_20$name

# Check the outcome with View(wb_scaled) - see that the rownames are now the country name (this is useful for visualizing)
```

Great, now we have a simplified, scaled version of the numeric variables, with rownames containing the county name. 

## Find the Euclidean distances

Use the `stats::dist()` function to find the Euclidean distance in multivariate space between the different observations (countries):

```{r}

# Compute dissimilarity values (Euclidean distances):
euc_distance <- dist(wb_scaled, method = "euclidean") ### add diag and upper

# Check out the output:
# euc_distance
```

Note: so like you saw in lecture, you *could* manually create the dendrogram using those distances! But it would take a pretty long time, so instead...

## Perform hierarchical clustering by complete linkage with `stats::hclust()`

The `stats::hclust()` function performs hierarchical clustering, given a dissimilarity matrix (our matrix of euclidean distances), using a linkage that you specify. 

Here, let's use complete linkage (recall from lecture: clusters are merged by the smallest *maximum* distance between two observations in distinct clusters).


```{r}

# Hierarchical clustering (complete linkage)
hc_complete <- hclust(euc_distance, method = "complete" )

# Plot it (base plot):
plot(hc_complete, cex = 0.6, hang = -1)

```

## Now let's do it by single linkage & compare

Let's update the linkage to single linkage (recall from lecture: this means that clusters are merged by the *smallest* distance between observations in separate clusters):

```{r}

# Hierarchical clustering (single linkage)
hc_single <- hclust(euc_distance, method = "single" )

# Plot it (base plot):
plot(hc_single, cex = 0.6, hang = -1)

```

We see that it is a bit different when we change the linkage! But how different? 

### Make a tanglegram to compare dendrograms 

Let's make a **tanglegram** to compare clustering by complete and single linkage! We'll use the `dendextend::tanglegram()` function to make it. 

First, we'll convert to class `dendrogram`, then combine them into a list:

```{r}
# Convert to class dendrogram
dend_complete <- as.dendrogram(hc_complete)
dend_simple <- as.dendrogram(hc_single)
```

Cool, now make a tanglegram: 

```{r}
# Make a tanglegram
tanglegram(dend_complete, dend_simple)
```

That allows us to compare how things are clustered by the different linkages!

Untangling:

```{r}
entanglement(dend_complete, dend_simple) # lower is better
#> [1] 0.3959222

untangle(dend_complete, dend_simple, method = "step1side") %>% 
  entanglement()
# [1] 0.06415907
```

Notice that just because we can get two trees to have horizontal connecting lines, it doesn’t mean these trees are identical (or even very similar topologically):

``` {r}
untangle(dend_complete, dend_simple, method = "step1side") %>% 
   tanglegram(common_subtrees_color_branches = TRUE)
```


## Want to plot your dendrogram with ggplot instead? Me too. 

Here's how you can make your dendrogram with `ggplot` (here, I'll use the complete linkage example stored as `hc_complete`) using `ggdendrogram()`, a `ggplot` wrapper: 

```{r}
ggdendrogram(hc_complete, 
             rotate = TRUE) +
  theme_minimal() +
  labs(x = "Country")

# COOL. Then you can customize w/ usual ggplot tools. 
```

## End Clustering lab
